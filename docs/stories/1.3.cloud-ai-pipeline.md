# Story 1.3: Cloud AI Pipeline (STT → LLM → TTS)

## Status
**Ready for Testing**

---

## Story

**As a** user,  
**I want** Arvis to understand my voice commands and respond with a calm, natural voice,  
**so that** I can control my room hands-free with natural language.

---

## Acceptance Criteria

1. STTBackend transcribes audio using OpenAI Whisper API
2. LLMBackend extracts intent from text using GPT-4o-mini
3. LLM returns structured JSON with action and params
4. TTSBackend synthesizes speech using OpenAI TTS with "onyx" voice
5. AudioController plays TTS audio through speakers
6. Full pipeline completes in <2.5 seconds (target latency)
7. Errors are handled gracefully with user feedback ("I'm offline", etc.)
8. VoiceAgent orchestrates the full flow: audio → STT → LLM → TTS
9. Intent is published as event for downstream processing
10. Unit tests verify each backend independently

---

## Tasks / Subtasks

- [ ] **Task 1: Implement STTBackend** (AC: 1)
  - [ ] Create `src/backends/stt_backend.py`
  - [ ] Implement `STTBackend` class
  - [ ] Implement `transcribe(audio_bytes) -> str` method
  - [ ] Use OpenAI Whisper API (`/audio/transcriptions`)
  - [ ] Handle API errors gracefully
  - [ ] Log transcription results

- [ ] **Task 2: Implement LLMBackend** (AC: 2, 3)
  - [ ] Create `src/backends/llm_backend.py`
  - [ ] Implement `LLMBackend` class
  - [ ] Implement `extract_intent(text, context) -> Intent` method
  - [ ] Create system prompt for intent extraction (from architecture)
  - [ ] Use GPT-4o-mini model
  - [ ] Parse JSON response into Intent object
  - [ ] Handle unclear commands with clarification intent

- [ ] **Task 3: Implement TTSBackend** (AC: 4)
  - [ ] Create `src/backends/tts_backend.py`
  - [ ] Implement `TTSBackend` class
  - [ ] Implement `synthesize(text) -> bytes` method
  - [ ] Use OpenAI TTS API (`/audio/speech`)
  - [ ] Set voice to "onyx"
  - [ ] Set model to "tts-1" (faster) or "tts-1-hd" (quality)
  - [ ] Return audio bytes (mp3 format)

- [ ] **Task 4: Implement AudioController** (AC: 5)
  - [ ] Create `src/controllers/audio_controller.py`
  - [ ] Implement `AudioController` class
  - [ ] Implement `say(text)` method (calls TTS, plays audio)
  - [ ] Implement `play_audio(audio_bytes)` method
  - [ ] Use pygame.mixer for playback
  - [ ] Handle playback errors

- [ ] **Task 5: Wire VoiceAgent pipeline** (AC: 8, 9)
  - [ ] Update `src/agents/voice_agent.py`
  - [ ] After recording complete:
    1. Call `stt_backend.transcribe(audio)`
    2. Call `llm_backend.extract_intent(text)`
    3. Publish `voice.command` event with intent
    4. (TTS response handled by ActionExecutor in next story)
  - [ ] Log each step with timing

- [ ] **Task 6: Implement error handling** (AC: 7)
  - [ ] Create error response mapping in config
  - [ ] On network error: say "I'm offline."
  - [ ] On API error: say "Something went wrong."
  - [ ] On unclear command: say "I didn't catch that."
  - [ ] Implement retry logic (1 silent retry)

- [ ] **Task 7: Measure and optimize latency** (AC: 6)
  - [ ] Add timing logs for each pipeline step
  - [ ] Measure end-to-end latency
  - [ ] Optimize if needed (parallel calls, caching)

- [ ] **Task 8: Write tests** (AC: 10)
  - [ ] Create `tests/test_stt_backend.py`
  - [ ] Create `tests/test_llm_backend.py`
  - [ ] Create `tests/test_tts_backend.py`
  - [ ] Create `tests/test_audio_controller.py`
  - [ ] Mock API calls in tests
  - [ ] Test error handling paths

---

## Dev Notes

### Source Tree Reference
```
src/
├── backends/
│   ├── stt_backend.py      # NEW
│   ├── llm_backend.py      # NEW
│   └── tts_backend.py      # NEW
├── controllers/
│   └── audio_controller.py # NEW
└── agents/
    └── voice_agent.py      # UPDATE
```

### OpenAI API Endpoints

**Whisper STT:**
```python
response = client.audio.transcriptions.create(
    model="whisper-1",
    file=audio_file
)
```

**GPT Intent Extraction:**
```python
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": INTENT_PROMPT},
        {"role": "user", "content": user_text}
    ],
    response_format={"type": "json_object"}
)
```

**TTS:**
```python
response = client.audio.speech.create(
    model="tts-1",
    voice="onyx",
    input=text
)
audio_bytes = response.content
```

### LLM System Prompt
See architecture document Appendix A for full prompt.

### Key Events
- `voice.command` — payload: `{text, intent: {action, params}}`

### Dependencies to Add
```
openai>=1.0.0
httpx>=0.25.0
pygame>=2.5.0
```

### Latency Breakdown Target
| Step | Target |
|------|--------|
| STT | <1.0s |
| LLM | <0.8s |
| TTS | <0.5s |
| Playback | <0.2s |
| **Total** | **<2.5s** |

---

## Testing

- **Test location**: `tests/`
- **API Mocking**: Use `unittest.mock` to mock OpenAI client
- **Test audio**: Use short test WAV file
- **Run tests**: `pytest tests/test_*_backend.py -v`

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2024-11-30 | 1.0 | Initial story creation | Sarah (PO) |

